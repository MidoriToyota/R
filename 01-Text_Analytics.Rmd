---
title: "**Text Analytics**"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style type="text/css">

h1.title {
  font-size: 30px;
  color: DarkBlue;
}
h1 { /* Header 1 */
  font-size: 26px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: darkred;
}
</style>

# **Carregamento e conversão dos dados**

## **Carregando um pacote que não é do CRAN**
```{r, include=FALSE}
# if (!requireNamespace("BiocManager", quietly = TRUE))  
# install.packages("BiocManager")           # Biocondutor
# BiocManager::install("Rgraphviz")         # Plots de correlação entre palavras    
```

## **Instalando os pacotes**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
library(tm)                           # Framework de text mining
library(qdap)                         # Análise quantitativa do discurso de trasncripts
library(qdapDictionaries)             # Dicionários
library(dplyr)                        # Manipulação de Dados 
library(RColorBrewer)                 # Paleta de cores 
library(ggplot2)                      # Gráficos 
library(scales)                       # Permite incluir vírgulas em números, criando milhares  
library(Rgraphviz) 
library(SnowballC)
library(stringr)
```

## **Visualizando as fontes**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
getSources()
getReaders()
```

## **Listar arquivos pdf - list.files()**

Criar vetor com o nome dos arquivos. Vamos listar todos os arquivos dentro do path e que sejam de formado PDF e salvar dentro de myfiles.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
myfiles = list.files(path = "C:/Users/Kamila Midori Toyota/OneDrive/00. Entregas/26. Curso_DSA_Business_Analytics/08-Text_Analytics/Material/pdf", pattern = "pdf",  full.names = TRUE)
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
head(myfiles,3)
```

## **PDF para txt - system2(pdftotext)**

Converter PDF para txt. Vamos pegar cada arquivo de myfiles e usar a função i. A função i foi feita para executar o programa "pdftotext" em um arquivo. O comando **system2** chama um programa executável e aplica à arquivos.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, results='hide'}
lapply(myfiles, function(i) system2("C:/Program Files/Xpdf/bin64/pdftotext", paste0('"', i, '"'), wait = FALSE))
```

Endereço das pastas de origem e destino
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
current.folder = "C:/Users/Kamila Midori Toyota/OneDrive/00. Entregas/26. Curso_DSA_Business_Analytics/08-Text_Analytics/Material/pdf"
new.folder = "C:/Users/Kamila Midori Toyota/OneDrive/00. Entregas/26. Curso_DSA_Business_Analytics/08-Text_Analytics/Material/txt"
```

Listar o nome dos arquivos em txt que foram criados e se encontram na pasta de pdfs.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
list.of.files = list.files(current.folder, "*.txt$", full.names = T)
head(list.of.files,3)
```

## **Criar pasta para arquivos txt - dir.create()**

Criando a pasta e copiando os arquivos:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, results='hide'}
dir.create(new.folder, recursive = TRUE)            # Criar diretório
file.copy(from = list.of.files, to = new.folder)    # Copiar txts para o diretório
```
cname = file.path(".","Material","txt"). Essa é uma opção para criar o caminho do diretório, cname é o equivalente à new.folder.

## **Lista de arquivos dentro do diretório new.folder - dir()**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
dir(new.folder)
```

## **Criar uma coleção de documentos de texto - Corpus(DirSource())**

A directory source acquires a list of files via dir and interprets each file as a document.

A função **Corpus()** representa e computa em corpora. Corpora are collections of documents containing (natural language) text. Vamos resumir:

- new.folder é um caminho.
- DirSource pega uma lista de arquivos dentro de um caminho e lê cada arquivo
- Corpus pega um arquivo 


```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
docs = Corpus(DirSource(new.folder))
```

## **Ler um arquivo - inspect()**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
class(docs)          
class(docs[[1]])
head(summary(docs))        # Mostra nomes dos documentos, legth e mode = list
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, results='hide'}
inspect(docs[16])          # Abre o conteúdo do documento nº 16
```

Caso eu queira utilizar o documento diretamente com o PDF ou DOC, posso utilizar o código abaixo, mas é muito mais trabalhoso e gasta muito processamento:

- docs <- Corpus(DirSource(cname), readerControl=list(reader=readPDF))
- docs <- Corpus(DirSource(cname), readerControl=list(reader=readDOC))

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE, results='hide'}
#a = sapply(docs, function(x){
# enc2utf8(x)})
```


# **Pre-processamento**

## **Lista de funções de transformação**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
getTransformations()
```

## **Limpando os dados**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
toSpace = content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs = tm_map(docs, toSpace, "/")
docs = tm_map(docs, toSpace, "@")
docs = tm_map(docs, toSpace, "\\|")
docs = tm_map(docs, toSpace, "/|@|\\|")
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removePunctuation)
```

## **Stopwords**

**Lista de stopwords**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
length(stopwords("english"))
head(stopwords("english"),12)
```

## **Removendo palavras - tm_map()**

A função tm_map() aplica transformações em palabras. 

Vamos atualizar os documentos removendo as stopwords, palavras específicas e tirando múltiplos espaços (multiplos espaços juntos viram um espaço só) 
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, removeWords, c("department", "email", "aaa"))
docs = tm_map(docs, stripWhitespace)
```

## **Transformando texto em siglas - content_transformer()**

A função content_transforme Create content transformers, i.e., functions which modify the content of an R object.

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
toString = content_transformer(function(x, from, to) gsub(from, to, x))
docs = tm_map(docs, toString, "harbin institute technology", "HIT")
docs = tm_map(docs, toString, "shenzhen institutes advanced technology", "SIAT")
docs = tm_map(docs, toString, "chinese academy sciences", "CAS")
```

## **Stemming (Eliminação de prefixos e sufixos)**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
docs = tm_map(docs, stemDocument)
```

# **Processamento**

## **Matriz de termo de documento - DocumentTermMatrix()**

Os computadores não são bons em processar palavras e letras, mas sim números.
Uma matriz de termo de documento é simplesmente uma matriz com documentos como as linhas e termos como as colunas E uma contagem da frequência de palavras como as células da matriz. 

Usamos DocumentTermMatrix() para criar a matriz:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
dtm = DocumentTermMatrix(docs)
```

A matriz do termo do documento é de fato bastante esparsa (isto é, na maior parte vazia) e assim é realmente armazenada em uma representação muito mais compacta internamente. 

Podemos ainda obter a contagem de linhas e colunas:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
dim(dtm)
class(dtm)
inspect(dtm[1:5, 1000:1005])
```

## **Matriz transposta - TermDocumentMatrix()**

Criar a matriz transposta com o TermDocumentMatrix():
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
tdm = TermDocumentMatrix(docs)
inspect(tdm[1000:1005,1:5])
```

## **Frequência dos termos**

Podemos obter as frequências dos termos como um vetor convertendo a matriz do termo do documento em uma matriz e somando as contagens das colunas:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
freq = colSums(as.matrix(dtm))
length(freq)
ord = order(freq)
freq[head(ord)]
freq[tail(ord)]
```

Distribuição das Frequências dos Termos:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
head(table(freq), 15)
tail(table(freq), 15)
```

## **Salvando resultado em um arquivo cvs**

Convertendo a Matriz para csv e salvando:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
write.csv(as.matrix(dtm), file="Material/dtm.csv")
```

## **Removendo termos esparsos - removeSparseTerms()**

Muitas vezes não estamos interessados em termos infrequentes em nossos documentos. 
Tais termos "esparsos" podem ser removidos da matriz do termo do documento muito facilmente usando removeSparseTerms ():
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
dim(dtm)
dtms = removeSparseTerms(dtm, 0.1)
dim(dtms)
inspect(dtms)
```

Podemos ver o efeito observando os termos que deixamos:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
freq = colSums(as.matrix(dtms))
freq
```

## **Identificando termos frequentes - findFreqTerms()**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
findFreqTerms(dtm, lowfreq = 1000)
findFreqTerms(dtm, lowfreq = 200)
```

## **Identificando associações - findAssocs()**

Buscando associações com palavras e especificando o limite de correlação
Se duas palavras aparecerem sempre juntas então a correlação seria 1.0 e se elas nunca aparecerem a correlação seria de 0,0. Assim, a correlação é uma medida de quão as palavras estão juntas no corpus.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
findAssocs(dtm, "data", corlimit = 0.6)
```

## **Plot de correlação - corThreshold**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 100)[1:50], corThreshold = 0.5, attrs=list(node=list(fontsize=22,fontcolor="red", shape="box", height=1, width=2)))
```

Para pegar todos os atributos que podem ser modificados para fazer a formatação da correlação utilizar: getDefaultAttrs(curAttrs = list())

## **Plot da frequência das palavras**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
freq = sort(colSums(as.matrix(dtm)), decreasing = TRUE)
head(freq, 14)
```

## **Plot de palavras mais frequentes**

Correlações:
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
wf = data.frame(word=names(freq), freq=freq)
head(wf)
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
data = subset(wf, freq>500)
ggplot(data = data, aes(x = reorder(word, -freq), freq, fill = freq)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# **Visualização de Dados**

## **Pacote wordcloud**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
library(wordcloud)
```

**Visualização 1**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
set.seed(142)
wordcloud(names(freq), freq, max.words = 100)
```

Preto e branco, sem muito destaque para as palavras mais importantes.

**Visualização 2**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
set.seed(142)
wordcloud(names(freq), freq, min.freq = 100, colors = brewer.pal(6, "Dark2"))
```
Falta corrigir o tamanho das palavras.

**Visualização 3**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
set.seed(142)
wordcloud(names(freq), freq, min.freq = 200, scale=c(5, .1), colors = brewer.pal(6, "Dark2"))
```

**Visualização 4**
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
set.seed(142)
dark2 = brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, min.freq = 100, rot.per = 0.2, colors = dark2)
# rot.per: proporção de palavras com rotação de 90º
```

## **Entendendo os dados em números de letras**

Retirando palavras muito longas (mais de 20 caracteres), provavelmente são fruto de algum erro.
```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
words = colnames(as.matrix(dtm))
length(words)
words = words[nchar(words) < 20]
length(words)
```

**Média de letras por palavra**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
head(words, 15)
length(words)
summary(nchar(words))
table(nchar(words))
dist_tab(nchar(words))
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
data = data.frame(nletters=nchar(words))
ggplot(data = data, aes(x=nletters)) +
  geom_histogram(binwidth=1) +
  geom_vline(xintercept=mean(nchar(words)), colour="green", size=1, alpha=.5)+ 
  labs(x="Numero de Letras", y="Numero de Palavras")
```

**Letras mais frequentes**

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
words %>%
  str_split("") %>%
  sapply(function(x) x[-1]) %>%
  unlist %>%
  dist_tab %>%
  mutate(Letter=factor(toupper(interval),
                       levels=toupper(interval[order(freq)]))) %>%
  ggplot(aes(Letter, weight=percent)) +
  geom_bar() +
  coord_flip() +
  labs(y="Proporcao") +
  scale_y_continuous(breaks=seq(0, 12, 2),
                     label=function(x) paste0(x, "%"),expand=c(0,0), limits=c(0,12))
```

```{r, echo=TRUE, message=FALSE, warning=FALSE, collapse=TRUE}
words %>%
  lapply(function(x) sapply(letters, gregexpr, x, fixed=TRUE)) %>%
  unlist %>%
  (function(x) x[x!=-1]) %>%
  (function(x) setNames(x, gsub("\\d", "", names(x)))) %>%
  (function(x) apply(table(data.frame(letter=toupper(names(x)),
                                      position=unname(x))),
                     1, function(y) y/length(x))) %>%
  qheat(high="green", low="yellow", by.column=NULL,
        values=TRUE, digits=3, plot=FALSE) +
  labs(y="Letra", x="Posicao") +
  theme(axis.text.x=element_text(angle=0)) +
  guides(fill=guide_legend(title="Proporcao"))
```





















